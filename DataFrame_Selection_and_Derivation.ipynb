{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekmanjunatha/abhishekmanjunatha.github.io/blob/main/DataFrame_Selection_and_Derivation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import ssl\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import * # Importing common PySpark SQL functions\n",
        "import sys\n",
        "\n",
        "# --- Environment Setup (Crucial for PySpark to run correctly) ---\n",
        "# Create directories for data and Hadoop binaries if they don't exist\n",
        "data_dir = \"data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "data_dir1 = \"hadoop/bin\"\n",
        "os.makedirs(data_dir1, exist_ok=True)\n",
        "\n",
        "# Define URLs for necessary files (test.txt, winutils.exe, hadoop.dll)\n",
        "# winutils.exe and hadoop.dll are often required for Spark to run on Windows\n",
        "urls_and_paths = {\n",
        "    \"https://raw.githubusercontent.com/saiadityaus1/SparkCore1/master/test.txt\": os.path.join(data_dir, \"test.txt\"),\n",
        "    # These files are specifically for Windows, and not needed on Linux\n",
        "    # \"https://github.com/saiadityaus1/SparkCore1/raw/master/winutils.exe\": os.path.join(data_dir1, \"winutils.exe\"),\n",
        "    # \"https://github.com/saiadityaus1/SparkCore1/raw/master/hadoop.dll\": os.path.join(data_dir1, \"hadoop.dll\")\n",
        "}\n",
        "\n",
        "# Create an unverified SSL context to handle potential SSL certificate issues\n",
        "ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "# Download necessary files if they don't exist\n",
        "for url, path in urls_and_paths.items():\n",
        "    if not os.path.exists(path): # Check if file already exists to avoid re-downloading\n",
        "        try:\n",
        "            with urllib.request.urlopen(url, context=ssl_context) as response, open(path, 'wb') as out_file:\n",
        "                out_file.write(response.read())\n",
        "            print(f\"Downloaded: {os.path.basename(path)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {os.path.basename(path)}: {e}\")\n",
        "    else:\n",
        "        print(f\"File already exists: {os.path.basename(path)}\")\n",
        "\n",
        "\n",
        "# Set environment variables for PySpark and Hadoop\n",
        "python_path = sys.executable # Get the path to the current Python executable\n",
        "os.environ['PYSPARK_PYTHON'] = python_path # Tell PySpark which Python interpreter to use\n",
        "# HADOOP_HOME might still be useful for some Spark features,\n",
        "# but winutils.exe and hadoop.dll are not relevant on Linux.\n",
        "os.environ['HADOOP_HOME'] = \"hadoop\" # Set Hadoop home directory\n",
        "\n",
        "# Set JAVA_HOME to the actual path of your JDK installation on Linux.\n",
        "# Replace '/path/to/your/jdk' with the correct path found in step 1.\n",
        "# Example: os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
        "# You might need to adjust this based on your specific Linux distribution and Java installation.\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64' # <-- **CHANGE THIS PATH**\n",
        "\n",
        "# Optional: Configuration for external Spark packages (commented out in your files)\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.1 pyspark-shell'\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.5.4 pyspark-shell'\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 pyspark-shell'\n",
        "\n",
        "# --- Spark Session and Context Initialization ---\n",
        "# Configure Spark application\n",
        "conf = SparkConf().setAppName(\"pyspark_learning\").setMaster(\"local[*]\") \\\n",
        "    .set(\"spark.driver.host\",\"localhost\") \\\n",
        "    .set(\"spark.default.parallelism\", \"1\")\n",
        "\n",
        "# Initialize SparkContext (low-level API, often managed by SparkSession)\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Initialize SparkSession (entry point for DataFrame and SQL functionalities)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print(\"Spark Session and Context initialized successfully!\")\n",
        "\n",
        "# Assuming 'spark' and 'df' from Phase 1 setup are already available.\n",
        "# Re-creating df for context in this example, but in your actual flow,\n",
        "# you'd continue using the 'df' from the previous step.\n",
        "\n",
        "data = [\n",
        "    (\"00000\", \"06-26-2011\", 200, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (\"00001\", \"05-26-2011\", 300, \"Exercise\", \"Weightlifting\", \"credit\"),\n",
        "    (\"00002\", \"06-01-2011\", 100, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (\"00003\", \"06-05-2011\", 100, \"Gymnastics\", \"Rings\", \"credit\"),\n",
        "    (\"00004\", \"12-17-2011\", 300, \"Team Sports\", \"Field\", \"paytm\"),\n",
        "    (\"00005\", \"02-14-2011\", 200, \"Gymnastics\", None, \"cash\")\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"------ORIGINAL DATAFRAME-------\")\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "print(\"============Dataframe with derived columns (procdf with selectExpr------------\")\n",
        "procdf = df.selectExpr(\n",
        "    \"id\",\n",
        "    \"split(tdate, '-')[2] as year\",\n",
        "    \"amount+100 as amount_increased\",\n",
        "    \"upper(category) as upper_cateroized\",\n",
        "    \"concat(product, '~zeyo') as product_suffix\",\n",
        "    \"spendby\",\n",
        "    \"case when spendby='cash' then 0 else 1 end as status\"\n",
        ")\n",
        "\n",
        "procdf.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: test.txt\n",
            "Spark Session and Context initialized successfully!\n",
            "------ORIGINAL DATAFRAME-------\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|   id|     tdate|amount|   category|      product|spendby|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|00000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|\n",
            "|00001|05-26-2011|   300|   Exercise|Weightlifting| credit|\n",
            "|00002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|\n",
            "|00003|06-05-2011|   100| Gymnastics|        Rings| credit|\n",
            "|00004|12-17-2011|   300|Team Sports|        Field|  paytm|\n",
            "|00005|02-14-2011|   200| Gymnastics|         NULL|   cash|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- tdate: string (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- spendby: string (nullable = true)\n",
            "\n",
            "============Dataframe with derived columns (procdf with selectExpr------------\n",
            "+-----+----+----------------+----------------+------------------+-------+------+\n",
            "|   id|year|amount_increased|upper_cateroized|    product_suffix|spendby|status|\n",
            "+-----+----+----------------+----------------+------------------+-------+------+\n",
            "|00000|2011|             300|        EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|00001|2011|             400|        EXERCISE|Weightlifting~zeyo| credit|     1|\n",
            "|00002|2011|             200|        EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|00003|2011|             200|      GYMNASTICS|        Rings~zeyo| credit|     1|\n",
            "|00004|2011|             400|     TEAM SPORTS|        Field~zeyo|  paytm|     1|\n",
            "|00005|2011|             300|      GYMNASTICS|              NULL|   cash|     0|\n",
            "+-----+----+----------------+----------------+------------------+-------+------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKJNZVJO8ssS",
        "outputId": "2c374f0b-0b29-4d94-a8af-be39e0f901bd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}